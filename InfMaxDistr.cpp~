/**
* @file InfMaxDistr.cpp
* @brief This project is used to demonstrate the experiments for the influence maximization in distributed manner
*
* @author Jing Tang (Nanyang Technological University)
*
* Copyright (C) 2017 Jing Tang and Nanyang Technological University. All rights reserved.
*
*/

#include "stdafx.h"
#include <time.h>
#include <iomanip>
int targetSize = 50;
#define NUM_PER_PROC 20000000
#define NATURAL_E 2.71828
//int buffer[NUM_PER_PROC];

int* modified_all = (int*)calloc(NUM_PER_PROC * 50, sizeof(int));
//int modified_all[5*NUM_PER_PROC];
int* rebuffer = (int*)calloc(NUM_PER_PROC * 50, sizeof(int));
//int rebuffer[10*NUM_PER_PROC];
int* cover_all = (int*)calloc(NUM_PER_PROC * 50, sizeof(int));
//int cover_all[10*NUM_PER_PROC] = { 0 };
int max_index = 0;
static  int  src = 0;

int *vecSeed=(int *)calloc(targetSize, sizeof(int));
int* modified_nodes = (int*)calloc(NUM_PER_PROC*2, sizeof(int));
double commutime = 0;
double LB = 1;
double epsilon = 0.1,l=1;
double epsilon_p = sqrt(2)*epsilon;
MPI_Request req;
MPI_Status status;
double getFr(int world_size,int num_rr_sets,int num_nodes)
{
	int ithSeed = 0;
	int num_rr_set_each = num_rr_sets / (world_size-1);
	for (int des = 1; des < world_size; des++) {
		MPI_Send(&num_rr_set_each, 1, MPI_INT, des, 0, MPI_COMM_WORLD);
	}
	Timer t_commu("");
	int max_nodes = 0, max_index = 0;
	double Fr = 0;
	//find index with maximum number of sets
	int num_degree;
	for (int source = 1; source < world_size; source++) {
		MPI_Recv(&num_degree, 1, MPI_INT, source, source, MPI_COMM_WORLD, &status);
		//if (num_nodes > max_num_nodes) {
		//	max_num_nodes = num_nodes;
		//}
		std::cout << "number of degrees from " << source << " is " << num_degree << std::endl;
		t_commu.refresh_time();
		MPI_Irecv(rebuffer, num_nodes, MPI_INT, source, source, MPI_COMM_WORLD, &req);
		MPI_Wait(&req, &status);
		commutime += t_commu.get_operation_time();
		//std::cout << "communication time from " << source << " is " << commutime << std::endl;
		for (int i = 0; i < num_nodes; i++) {
			cover_all[i] += rebuffer[i];
		}
	}
	//std::cout << "first round " << t.get_total_time() << std::endl;
	for (int i = 0; i < num_nodes; i++) {
		if (cover_all[i] > max_nodes) {
			max_nodes = cover_all[i];
			max_index = i;
		}
	}
	Fr += 1.0*cover_all[max_index] / num_rr_sets;
	//std::cout <<"parameters    !!!!!"<< cover_all[max_index]<<" "<<Fr <<" "<<num_rr_sets<< std::endl;
	std::vector<std::vector<int>> length_list(max_nodes + 1, std::vector<int>());
	for (int i = 0; i < num_nodes; i++) {
		length_list[cover_all[i]].push_back(i);
	}
	for (int des = 1; des < world_size; des++) {
		MPI_Send(&max_index, 1, MPI_INT, des, 0, MPI_COMM_WORLD);
	}
	int num_modified_nodes;
	int arg = 0;

	vecSeed[ithSeed++] = max_index;
	//std::cout << "number " << ithSeed << " iteration:index with maximum number of sets " << max_index << std::endl;
	for (int source = 1; source < world_size; source++) {
		t_commu.refresh_time();
		MPI_Recv(&num_modified_nodes, 1, MPI_INT, source, source, MPI_COMM_WORLD, &status);
		MPI_Irecv(modified_all, num_modified_nodes, MPI_INT, source, source, MPI_COMM_WORLD, &req);
		MPI_Wait(&req, &status);
		commutime += t_commu.get_operation_time();
		for (int i = 0; i < num_modified_nodes; i++) {
			cover_all[modified_all[i]]--;
		}
	}
	cover_all[max_index] = 0;
	max_nodes = 0;
	max_index = 0;
	length_list.pop_back();
	//		while (vecSeed.size() <= targetSize){
	for (auto deg = length_list.size() - 1; deg >= 0; deg--) {
		auto& node = length_list[deg];
		for (auto idx = 0; idx<node.size(); idx++) {
			arg = node[idx];
			auto currDeg = cover_all[arg];

			if (deg > currDeg) {
				if (currDeg <= 0) {
					continue;
				}
				else {
					length_list[currDeg].push_back(arg);
					continue;
				}
			}
			Fr += 1.0* cover_all[arg] / num_rr_sets;
			cover_all[arg] = 0;
			max_index = arg;
			vecSeed[ithSeed++] = max_index;
			//std::cout << "number " << ithSeed << " iteration:index with maximum number of sets " << max_index << std::endl;
			if (ithSeed == targetSize) {
				/*double tt = t.get_total_time();
				std::cout << " communication time is " << commutime << std::endl;
				std::cout << "total time used " << t.get_total_time();*/
				//std::ofstream f1;
				//f1.open("d:\\result.txt", std::ios::app);
				//if (!f1) return 0;
				////				f1 << std::setw(20) << "total time£º" << tt << std::endl;
				//f1 << std::setw(20) << tt << std::endl;
				////				f1 << std::setw(20) << "communication time£º" << commutime << std::endl;
				//f1 << std::setw(20) << commutime << std::endl;
				//f1.close();
				return Fr;
				//MPI_Finalize();
			}
			for (int des = 1; des < world_size; des++) {
				MPI_Send(&max_index, 1, MPI_INT, des, 0, MPI_COMM_WORLD);
			}
			//recv modified list / make changes
			for (int source = 1; source < world_size; source++) {
				t_commu.refresh_time();
				MPI_Recv(&num_modified_nodes, 1, MPI_INT, source, source, MPI_COMM_WORLD, &status);
				MPI_Irecv(modified_all, num_modified_nodes, MPI_INT, source, source, MPI_COMM_WORLD, &req);
				MPI_Wait(&req, &status);
				commutime += t_commu.get_operation_time();
				for (int i = 0; i < num_modified_nodes; i++) {
					cover_all[modified_all[i]]--;
				}
			}
		}
		length_list.pop_back();
	}
	return 0;
}

int main(int argc, char* argv[])
{
	MPI_Init(NULL, NULL);
	int rank;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	int world_size;
	MPI_Comm_size(MPI_COMM_WORLD, &world_size);
	MPI_Request req;
	MPI_Status status;
	std::ofstream f1;
	f1.open("result.txt", std::ios::app);
	if (!f1) return 0;
	f1 << std::endl;
	f1.close();
	if (rank == 0) {
		double max_load_time = 0;
		double graph_load_time = 0;
		for (int source = 1; source < world_size; source++) {
			MPI_Recv(&graph_load_time, 1, MPI_DOUBLE, source, source, MPI_COMM_WORLD, &status);
			if (graph_load_time > max_load_time) {
				max_load_time = graph_load_time;
			}
			std::cout << "graph loaded time from " << source << " is " << graph_load_time << std::endl;
		}
		int num_nodes = 0, max_nodes = 0;
		double x, theta=0;
		int num_rr_sets;
		double Fr = 0;
		MPI_Recv(&num_nodes, 1, MPI_INT, 1, 1, MPI_COMM_WORLD, &status);
		double alpha = sqrt(l*log(num_nodes) + log(2));
		double beta = sqrt((1 - 1 / NATURAL_E)*logcnk(num_nodes, targetSize) + l*log(num_nodes) + log(2));
		double lambda_p = (2 + 2 / 3 * epsilon_p)*(logcnk(num_nodes, targetSize) + l*log(num_nodes) + log(log2(num_nodes)))*num_nodes / pow(epsilon_p, 2);
		double lambda_s = 2 * num_nodes*pow2((1 - 1 / NATURAL_E)*alpha + beta)*pow2(1 / epsilon);
		Timer t_total("");
		for (int i = 1; i <= log2(num_nodes) - 1; i++)
		{
			x = num_nodes / pow(2, i);
			std::cout << "x = " << x << " lambda' = " << lambda_p << " number of nodes "<< int(lambda_p / x - theta) <<std::endl;
			Fr = getFr(world_size, int(lambda_p / x-theta), num_nodes);
			//std::cout << i << " th round Fr = " <<Fr<< std::endl;
			//std::cout << "1+epsilon_p)*x = " << (1 + epsilon_p)*x << std::endl;
			theta = lambda_p / x;
			std::cout << "theta = " << theta << std::endl;
			if(num_nodes*Fr>=(1+epsilon_p)*x)
			{
				LB = num_nodes*Fr / (1 + epsilon_p);
				break;
			}
		}
		double theta_p = lambda_s / LB;
		if(theta_p > theta)
		{
			Fr = getFr(world_size, int(theta_p - theta), num_nodes);
			std::cout << "end " << std::endl;
			std::cout << "total time is " << t_total.get_total_time() << std::endl;
			std::cout << "communication time " << commutime << std::endl;
		}
		double total_time = t_total.get_total_time();
		double t_rrset;		
		num_rr_sets = 0;
		for (int des = 1; des < world_size; des++) {
			MPI_Send(&num_rr_sets, 1, MPI_INT, des, 0, MPI_COMM_WORLD);
		}
		f1.open("result.txt", std::ios::app);
		if (!f1) return 0;
		f1 << std::setw(20) << total_time << std::endl;
		f1 << std::setw(20) << commutime<< std::endl;
		//std::cout << "total time is " << total_time << std::endl;
		//std::cout << "communication time " << commutime << std::endl;
		for (int source = 1; source < world_size; source++) {
			MPI_Recv(&t_rrset, 1, MPI_DOUBLE, source, source, MPI_COMM_WORLD, &status);
			std::cout << "rrset time is " << t_rrset << std::endl;
			f1 << std::setw(20) << t_rrset<< std::endl;
		}
		f1.close();
		MPI_Finalize();
		return 0;
	}

	//	}
	else {
		Timer t_worker("");
		// Randomize the seed for generating random numbers
		dsfmt_gv_init_gen_rand(static_cast<uint32_t>(time(nullptr)) + rank);
		TArgument Arg(argc, argv);
		std::string infilename = Arg._dir + "/" + Arg._graphname;
		if (Arg._func == 0 || Arg._func == 2)
		{// Format the graph
			GraphBase::format_graph(infilename, Arg._mode);
			if (Arg._func == 0) return 1;
		}
		std::cout << "---The Begin of " << Arg._outFileName << "---" << std::endl;
		//		Timer mainTimer("main");
				// Initialize a result object to record the results
		PResult pResult(new TResult());
		// Load the graph
		Graph graph = GraphBase::load_graph(infilename, true, Arg._probDist);
		if (Arg._model == TArgument::CascadeModel::LT)
		{// Normalize the propagation probabilities in accumulation format for LT cascade model for quickly generating RR sets
			to_normal_accum_prob(graph);
		}
		int numV = (int)graph.size();

		double lambda_p = (2 + 2 / 3 * epsilon_p)*(logcnk(numV, targetSize) + l*log(numV) + log(log2(numV)))*numV / pow(epsilon_p, 2);

		double* pCost;
		pCost = (double *)calloc(numV, sizeof(double));
		// Load the cost of each node
		//pCost = TIO::read_cost(infilename, numV, Arg._costDist, Arg._scale, Arg._para);
		//for (int i = 0; i < 20; i++) LogInfo(pCost[i]);
		double* pAccumWeight = nullptr; // Used for non-uniform benefit distribution for activating nodes
		bool isUniBen = true;
		if (tolower(Arg._benefitDist[0]) == 'n')
		{// Generate benefit weights for each node via normal distribution 
			isUniBen = false;
			/*Weighted benefit distribution*/
			std::default_random_engine generator;
			std::normal_distribution<double> distribution(3.0, 1.0);
			//std::binomial_distribution<int> distribution(100, 0.4);
			//std::poisson_distribution<int> distribution(1.0);
			//std::gamma_distribution<double> distribution(2.0, 2.0);
			//std::exponential_distribution<double> distribution(2.0);
			pAccumWeight = (double *)malloc(numV * sizeof(double));
			for (int i = 0; i < numV; i++)
			{
				pAccumWeight[i] = max(0, distribution(generator));
			}
			to_normal_accum_weight(pAccumWeight, numV);
		}
		// Create a hyper-graph object to generate/compute RR sets
		PHyperGraphRef pHyperG(new THyperGraphRef(graph, pAccumWeight));
		pHyperG->set_cascade_model(static_cast<THyperGraphRef::CascadeModel>(Arg._model));
		pHyperG->set_hyper_graph_mode(true);
		TAlg tAlg(pCost, pHyperG, pResult);
		std::cout << "  ==>Graph loaded for RIS! total time used (sec): " + std::to_string(t_worker.get_total_time()) << std::endl;
		//dsfmt_gv_init_gen_rand(0);
		double graph_load = t_worker.get_total_time();
		MPI_Send(&graph_load, 1, MPI_DOUBLE, 0, rank, MPI_COMM_WORLD);
		if (rank == 1) {
			MPI_Send(&numV, 1, MPI_INT, 0, rank, MPI_COMM_WORLD);
		}

		int num_rr_sets;
		MPI_Recv(&num_rr_sets, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
		// Build RR sets
		double t_RRset = 0;
		while (1)
		{
			t_worker.refresh_time();
			tAlg.build_n_RRsets(num_rr_sets);
			t_RRset += t_worker.get_operation_time();
			if(num_rr_sets == 0)
			{
				MPI_Send(&t_RRset, 1, MPI_DOUBLE, 0, rank, MPI_COMM_WORLD);
				break;
			}
			int *coverage;
			coverage = (int *)calloc(numV, sizeof(int));
			int degrees = 0;
			for (int i = 0; i < numV; i++)
			{
				auto deg = (int)pHyperG->_vecCover[i].size();
				coverage[i] = deg;
				degrees += deg;
				//			buffer[i] = deg;
				//			coverageHeap[i] = std::pair<int, int>(std::make_pair(deg, i));
				//			if (deg > largeDeg) largeDeg = deg;
			}
			//send coverage[i]

			MPI_Send(&degrees, 1, MPI_INT, 0, rank, MPI_COMM_WORLD);
			MPI_Isend(coverage, numV, MPI_INT, 0, rank, MPI_COMM_WORLD, &req);
			MPI_Wait(&req, &status);
			//receive max index 
			MPI_Recv(&max_index, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
			//		printf("first iteration recved index %d at process %d\n", max_index, rank);
			//		time1 = timerGreedy.get_operation_time();
			int argmaxIdx, iteration = 1;
			bool * edgeMark;
			edgeMark = (bool *)calloc(pHyperG->get_RRsets_size(), sizeof(bool));
			while (iteration < targetSize) {
				int num_modi = 0;
				//			modified_nodes.clear();
				argmaxIdx = max_index;
				//			int sumInf = 0;
				coverage[argmaxIdx] = 0;
				// check if an edge is removed	
				for (auto edgeIdx : pHyperG->_vecCover[argmaxIdx]) {
					if (edgeMark[edgeIdx]) continue;
					edgeMark[edgeIdx] = true;
					for (auto nodeIdx : pHyperG->_vecCoverRev[edgeIdx]) {
						coverage[nodeIdx]--;
						modified_nodes[num_modi] = nodeIdx;
						num_modi++;
					}
				}
				/*for (int i = 0; i < num_modi; i++) {
				modified_buf[i] = modified_nodes[i];
				}*/
				//			num_modi = modified_nodes.size();
				MPI_Send(&num_modi, 1, MPI_INT, 0, rank, MPI_COMM_WORLD);
				MPI_Isend(modified_nodes, num_modi, MPI_INT, 0, rank, MPI_COMM_WORLD, &req);
				MPI_Wait(&req, &status);
				if (iteration == targetSize - 1) {
					break;
				}
				MPI_Recv(&max_index, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
				//			printf("%d iteration recved index %d at process %d\n", iteration + 1, max_index, rank);
				iteration++;
			}
			MPI_Recv(&num_rr_sets, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
			//std::cout << "finish time of rank " << rank << " is " << std::to_string(t_worker.get_total_time()) << std::endl;
		}
		//std::ofstream f1;
		//f1.open("result.txt", std::ios::app);
		//if (!f1) return 0;
		////f1 << std::setw(20) << "build RR set from rank£º"<<rank <<" "<< t_RRset << std::endl;
		//f1 << std::setw(20) << t_RRset << std::endl;
		//f1.close();
	}
	MPI_Finalize();
	return 0;
}
